{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4436554-f296-4a1c-af0f-fedf55ac4879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd '/Users/yashchavan/Desktop/Machine_learning/License_plate_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fd1e12-fe5a-4859-b764-ea6c551b67fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pip\n",
    "# pip.main([\"install\", \"opencv-python\",'pyyaml'])\n",
    "# !pip3 install ipykernel --upgrade\n",
    "# !python3 -m ipykernel install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8e3a29-9b28-4fd3-aec6-929a2ab8d273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "# %cd /Users/yashchavan/Desktop/Machine_learning/License_plate_project/yolov5\n",
    "# # !python -m pip install -qr requirements.txt # install dependencies\n",
    "# # !pip3 install -q roboflow\n",
    "# # !pip3 install opencv-python\n",
    "# # !conda install yaml\n",
    "\n",
    "# import torch\n",
    "# import os\n",
    "# from IPython.display import Image, clear_output  # to display images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9956988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf1d0d3-3521-4c5e-a5a9-1374aaa322ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91c5e08-850b-4303-92ca-dcbacdb5b687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.environ[\"DATASET_DIRECTORY\"] = \"/Users/yashchavan/Desktop/Machine_learning/License_plate_project/datasets\"\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"9fHmslgy5tiXsOob0doQ\")\n",
    "# project = rf.workspace().project(\"license_plate-chwbc\")\n",
    "# dataset = project.version(3).download(\"yolov5\",location=\"/Users/yashchavan/Desktop/Machine_learning/License_plate_project/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e907cd18-553b-49b2-87ba-e6fc911cb454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python train.py --img 1280 --batch 10 --epochs 100 --data /Users/yashchavan/Desktop/Machine_learning/License_plate_project/yolov5/data.yaml --hyp hyp.scratch-med.yaml --cache # --weights yolov5m6.pt  # --cfg yolov5m.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce596eb1-e0e4-4838-aed9-56a0c9b43577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python detect.py --weights /Users/yashchavan/Desktop/Machine_learning/License_plate_project/yolov5/prev_trained/best.pt --img 1280 --source /Users/yashchavan/Desktop/Machine_learning/License_plate_project/datasets/test/images --save-crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79df0145-a963-42a6-a28c-513e9fbd3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import CTCLoss\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "char_list = string.ascii_letters+string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9424be1-8845-480c-99e7-251f236205e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input shape (height=32,weigth = 128)\n",
    "class convnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convnet, self).__init__()\n",
    "\n",
    "        self.conv_part = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=64, kernel_size=(3,3), stride=1, padding=\"same\"),#conv1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), stride=1, padding=\"same\"),#conv2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), stride=1, padding=\"same\"),#conv3\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=1, padding=\"same\"),#conv4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3,3), stride=1, padding=\"same\"),#conv5\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.BatchNorm2d(512,512),\n",
    "            # nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3,3), stride=1, padding=\"same\"),#conv6\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.25),\n",
    "            nn.BatchNorm2d(512,512),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(2,2), stride=1),#conv7\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.25),  \n",
    "        )\n",
    "\n",
    "        self.RNN_part1 = nn.Sequential(\n",
    "            nn.LSTM(input_size =512,batch_first=True,hidden_size=128,bidirectional=True,dropout=0.2,num_layers = 2),\n",
    "        )\n",
    "        self.RNN_part2 = nn.Sequential(\n",
    "            nn.LSTM(input_size =256,batch_first=True,hidden_size=128,bidirectional=True,dropout=0,num_layers = 2),\n",
    "        )\n",
    "        \n",
    "        self.CTC_part = nn.Sequential(\n",
    "            nn.Linear(256,len(char_list)+1),\n",
    "            nn.Softmax(dim=2)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_part(x)\n",
    "        # for layer in self.conv_part:\n",
    "        #     print(type(layer),\" : \",end=\"\")\n",
    "        #     x = layer(x)x\n",
    "            # print(x.size())\n",
    "        # print(out.shape)\n",
    "        # print(out)\n",
    "        batch_size, num_channels, height, width = out.shape\n",
    "        # out = out.permute(3, 0, 1, 2)\n",
    "        # # print(out.size(1))\n",
    "        # out = out.reshape((batch_size, out.size(1),-1))\n",
    "        # # out = out.view(width, batch_size, num_channels * height)\n",
    "        # # out = out.permute(1, 0, 2)\n",
    "        # # b, c, h, w = out.shape\n",
    "        # # assert h == 1, \"the height of conv must be 1\"\n",
    "        out = out.squeeze(2)\n",
    "        # print(out.shape)\n",
    "        out = out.permute(0,2,1)\n",
    "        # out = out.permute(2,0,1)\n",
    "\n",
    "        # print(out.shape)\n",
    "        # out = out.permute(2, 0, 1)  # [w, b, c]\n",
    "        # out = out.view(1,b,c*h*w)\n",
    "        # print(out.shape)\n",
    "        out, _= self.RNN_part1(out)\n",
    "        out,_ = self.RNN_part2(out)\n",
    "        # print(out.shape)\n",
    "        # x = x\n",
    "        # for layer in self.conv_part:\n",
    "        #     x = layer(x)\n",
    "        #     print(type(layer),\" : \",x.size())\n",
    "        # out, _= self.RNN_part2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.CTC_part(out)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "def ctc_decode(outputs):\n",
    "    # Get the predicted labels using the CTC decoding algorithm\n",
    "    _, preds = outputs.max(2)\n",
    "\n",
    "    # Collapse repeated labels and remove blank symbols\n",
    "    labels = []\n",
    "    for pred in preds:\n",
    "        import itertools\n",
    "        collapsed = [p for p, _ in itertools.groupby(pred) if p != len(CHARS)]\n",
    "        labels.append(collapsed)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e617f7-3574-4403-a53d-27660fc12d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# img = cv2.imread(\"/Users/yashchavan/Desktop/Machine_learning/License_plate_project/text_detect/croped/img_63.jpg\")\n",
    "# img = cv2.resize(img,(32,128))\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# img = img.T\n",
    "# img = img.reshape((-1,1,32,128))\n",
    "# img = torch.tensor(img).float()\n",
    "# model = convnet()\n",
    "# out = model(img)\n",
    "# out = out.detach().numpy()\n",
    "# print(out)\n",
    "# cv2.imshow(\"image\",out)\n",
    "# cv2.waitKey(0)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d03cfb-1823-4a79-8e19-56fafb2d73cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from  scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "data_img = []\n",
    "data_label = [] \n",
    "data_encoded = []\n",
    "map_label = {}\n",
    "data_img_val = []\n",
    "data_label_val = [] \n",
    "data_encoded_val = []\n",
    "dirc_val_file = \"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px/annotation_val.txt\"\n",
    "dirc_lexi = \"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px/lexicon.txt\"\n",
    "dirc_train_file = \"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px/annotation_train.txt\"\n",
    "dirc_train_imgs = \"/content/drive/MyDrive/ML_projects/License_plate_project/IIIT5K/train\"\n",
    "path_weight = \"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px/saved_w.txt\"\n",
    "\n",
    "train = pd.read_csv(dirc_train_file)\n",
    "\n",
    "with open(dirc_lexi) as f:\n",
    "    for i, text in enumerate(f.readlines()):\n",
    "        map_label[str(i)] = str(text).rstrip(\"\\n\\r\")\n",
    "# print(map_label)\n",
    "directory = r\"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px\"\n",
    "os.chdir(directory)\n",
    "with open(dirc_train_file) as f:\n",
    "    count = 0\n",
    "    for text in (f.readlines()):\n",
    "        if(count >=7000000):\n",
    "            break\n",
    "        temp = text.split(\" \")\n",
    "        # print(temp)\n",
    "        img = None\n",
    "        try:\n",
    "            img = cv2.imread(temp[0],cv2.IMREAD_COLOR)\n",
    "        except IOError:\n",
    "            print('Corrupted image for %d' % index)\n",
    "            continue\n",
    "        if(not np.any(img)):\n",
    "            continue\n",
    "        img = cv2.imread(temp[0],cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img,(128,32))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        data_img.append(img)\n",
    "        data_label.append(map_label[temp[-1].rstrip(\"\\n\\r\")])\n",
    "        count+=1\n",
    "    # print(count)\n",
    "data_img = np.array(data_img)\n",
    "# print(data_names)\n",
    "# print(data_img.shape)\n",
    "# print(map_name_idx)\n",
    "CHARS = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "CHAR2LABEL = {char: i  for i, char in enumerate(CHARS)}\n",
    "LABEL2CHAR = {label: char for char, label in CHAR2LABEL.items()}\n",
    "max_l = 0\n",
    "for i in range(len(data_label)):\n",
    "    t = []\n",
    "    # print(data_label[i])\n",
    "    for j in range(len(data_label[i])):\n",
    "        t.append(CHAR2LABEL[data_label[i][j]])\n",
    "    data_encoded.append(t)\n",
    "    max_l = max(max_l,len(t))\n",
    "# print(data_encoded)\n",
    "# print(t)\n",
    "\n",
    "for i in range(len(data_encoded)):\n",
    "    while len(data_encoded[i]) < max_l:\n",
    "        data_encoded[i].append(len(CHARS))\n",
    "data_encoded = np.array(data_encoded)\n",
    "# print(data_encoded.shape)\n",
    "\n",
    "def convert_towords(output):\n",
    "    out = []\n",
    "    for lis in output:\n",
    "        s = \"\"\n",
    "        for i in lis:\n",
    "            s+=str(LABEL2CHAR[int(i)])\n",
    "        out.append(s)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbee9d-de10-4e3f-9379-14046b8f3bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directory = r\"/Users/yashchavan/Downloads/datas/mnt/ramdisk/max/90kDICT32px\"\n",
    "# os.chdir(directory)\n",
    "with open(dirc_val_file) as f:\n",
    "    count = 0\n",
    "    for text in (f.readlines()):\n",
    "        if(count >=700000):\n",
    "            break\n",
    "        temp = text.split(\" \")\n",
    "        # print(temp)\n",
    "        img = None\n",
    "        try:\n",
    "            img = cv2.imread(temp[0],cv2.IMREAD_COLOR)\n",
    "        except IOError:\n",
    "            print('Corrupted image for %d' % index)\n",
    "            continue\n",
    "        if(not np.any(img)):\n",
    "            continue\n",
    "            \n",
    "        img = cv2.imread(temp[0],cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img,(128,32))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        data_img_val.append(img)\n",
    "        data_label_val.append(map_label[temp[-1].rstrip(\"\\n\\r\")])\n",
    "        count+=1\n",
    "data_img_val = np.array(data_img_val)\n",
    "max_l = 0\n",
    "for i in range(len(data_label_val)):\n",
    "    t = []\n",
    "    for j in range(len(data_label_val[i])):\n",
    "        t.append(CHAR2LABEL[data_label_val[i][j]])\n",
    "    data_encoded_val.append(t)\n",
    "    max_l = max(max_l,len(t))\n",
    "\n",
    "for i in range(len(data_encoded_val)):\n",
    "    while len(data_encoded_val[i]) < max_l:\n",
    "        data_encoded_val[i].append(len(CHARS))\n",
    "data_encoded_val = np.array(data_encoded_val)\n",
    "print(data_img_val.shape,data_encoded_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb11b3-d20c-4f16-9209-220f4299194f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "train_tensor = torch.utils.data.TensorDataset(torch.from_numpy(data_img),torch.from_numpy( data_encoded) )\n",
    "train_loader = torch.utils.data.DataLoader(train_tensor, batch_size = batch_size,shuffle = True)\n",
    "val_tensor = torch.utils.data.TensorDataset(torch.from_numpy(data_img_val),torch.from_numpy( data_encoded_val) )\n",
    "val_loader = torch.utils.data.DataLoader(val_tensor, batch_size = batch_size,shuffle = False)\n",
    "# from sys import getsizeof\n",
    "\n",
    "# print(getsizeof(data_img)/ 1024 / 1024,getsizeof(data_encoded)/ 1024 / 1024,getsizeof(data_label)/ 1024 / 1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298db26-e244-4176-9516-5b93c2fc56a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "import time\n",
    "# batch_size = 4\n",
    "ctc_loss = CTCLoss(blank=len(char_list), reduction='mean', zero_infinity=True)\n",
    "model = convnet()\n",
    "model.to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0005)\n",
    "val_loss = 1e5\n",
    "for epoch in range(epochs):\n",
    "    st = time.time()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    # for j in range(0,data_img.shape[0],2):\n",
    "        model.train()\n",
    "        local_X = data.view(data.shape[0],1,32,128).float()\n",
    "        local_X = local_X.to(\"cuda\")\n",
    "        outputs = model(local_X)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "        ll = torch.nn.LogSoftmax(dim=2)\n",
    "        \n",
    "#         outputs = ll(outputs)\n",
    "        outputs = torch.log(outputs)\n",
    "\n",
    "        targets = target.to(\"cuda\")\n",
    "        outputs = outputs.transpose(0,1).float()\n",
    "        output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.shape[0], dtype=torch.long).to(\"cuda\")\n",
    "        target_lengths = torch.full(size=(targets.size(0),), fill_value=targets.shape[1], dtype=torch.long).to(\"cuda\")\n",
    "        # outputs = outputs.to(\"cuda\")\n",
    "        loss = ctc_loss(outputs, targets, output_lengths, target_lengths)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "#         print(batch_idx,loss.item())\n",
    "#         break\n",
    "    temp_loss = 0\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "    # for j in range(0,data_img.shape[0],2):\n",
    "        local_X = data.view(data.shape[0],1,32,128).float()\n",
    "        local_X = local_X.to(\"cuda\")\n",
    "        outputs = model(local_X)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "#         outputs =  torch.nn.LogSoftmax(outputs,dim=2)\n",
    "        ll = torch.nn.LogSoftmax(dim=2)\n",
    "#         outputs = ll(outputs)\n",
    "        outputs = torch.log(outputs)\n",
    "\n",
    "        targets = target.to(\"cuda\")\n",
    "        outputs = outputs.transpose(0,1).float()\n",
    "        output_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.shape[0], dtype=torch.long).to(\"cuda\")\n",
    "        target_lengths = torch.full(size=(targets.size(0),), fill_value=targets.shape[1], dtype=torch.long).to(\"cuda\")\n",
    "        # outputs = outputs.to(\"cuda\")\n",
    "        loss = ctc_loss(outputs, targets, output_lengths, target_lengths)\n",
    "        temp_loss += loss.item()\n",
    "#         print(batch_idx,loss.item())\n",
    "#         break\n",
    "    if(temp_loss<val_loss):\n",
    "        torch.save(model.state_dict(), path_weight)\n",
    "    val_loss = temp_loss\n",
    "    total_loss = total_loss\n",
    "    print(epoch,\" : \", \"train_loss : \",total_loss, \", val_loss: \",val_loss,time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49698542-76a1-454c-86fb-f7584ecd2aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 24, 23, 23, 14, 14, 29, 18, 18, 18, 29, 18, 21, 18, 62, 62, 62, 62,\n",
      "         62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]], device='cuda:0')\n",
      "tensor([[[1.6256e-08, 1.7001e-08, 1.6257e-08,  ..., 1.5312e-08,\n",
      "          1.6589e-08, 2.2160e-05],\n",
      "         [1.4112e-08, 1.4393e-08, 1.3981e-08,  ..., 1.4065e-08,\n",
      "          1.3694e-08, 2.4829e-05],\n",
      "         [2.7451e-08, 2.9460e-08, 2.9233e-08,  ..., 2.9304e-08,\n",
      "          2.8409e-08, 5.1494e-03],\n",
      "         ...,\n",
      "         [2.1379e-10, 3.1600e-10, 2.4112e-10,  ..., 2.1626e-10,\n",
      "          1.9660e-10, 9.9976e-01],\n",
      "         [2.1273e-10, 2.7490e-10, 2.2535e-10,  ..., 1.9819e-10,\n",
      "          1.9237e-10, 1.0000e+00],\n",
      "         [2.0951e-11, 2.5241e-11, 2.2552e-11,  ..., 1.9200e-11,\n",
      "          1.8723e-11, 9.9998e-01]]], device='cuda:0')\n",
      "['conetitili']\n"
     ]
    }
   ],
   "source": [
    "# model.to(\"cuda\")\n",
    "# model.load_state_dict(torch.load(\"/Volumes/NO NAME/ml/w.txt\"))\n",
    "# model.to(\"cpu\")\n",
    "model.eval()\n",
    "img = cv2.imread(\"/home/aimlgpu2/Amol/YASH/mnt/ramdisk/max/90kDICT32px/1/1/117_wrecks_87388.jpg\")\n",
    "img = cv2.resize(img,(32,128))\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "img = img.T\n",
    "# img = img.reshape((-11,32,128))\n",
    "local_X = torch.tensor([img]).view(1,1,32,128).float().to(\"cuda\")\n",
    "out = model(local_X)\n",
    "# out = out.argmax(dim=2)\n",
    "out = out.detach()\n",
    "print(out.argmax(2))\n",
    "pred = ctc_decode(out) \n",
    "pred = convert_towords(pred)\n",
    "print(out)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08266a5c-f471-4545-acb0-d64541aacbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bd660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b4e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
