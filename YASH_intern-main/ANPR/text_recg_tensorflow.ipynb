{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18556,
     "status": "ok",
     "timestamp": 1685335479247,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "yNFYesYew4q0",
    "outputId": "97acdd5e-64f4-43b8-eeee-dd477c9dec75"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1685338500021,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "kKxLfU-vylAp",
    "outputId": "ca4ee342-9ddc-42a9-f4a7-29905940d3f4"
   },
   "outputs": [],
   "source": [
    "# %cd \"/content/drive/MyDrive/ML_projects/License_plate_project/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ons0Gnbz__lw",
    "outputId": "1560e269-cb5a-42ea-ebc8-0558e39ac734"
   },
   "outputs": [],
   "source": [
    "# !tar -xvzf mjsynth.tar.gz -C \"/content/drive/MyDrive/ML_projects/License_plate_project/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8566,
     "status": "ok",
     "timestamp": 1685335487809,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "OFYt-7auznjq",
    "outputId": "2ecff749-7ce4-43e6-a740-81a60e7e9a58"
   },
   "outputs": [],
   "source": [
    "# !pip install python-h5py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uG58h2h4xAjB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 15:55:49.625391: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-01 15:55:49.673801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (/home/aimlgpu2/.local/lib/python3.9/site-packages/google/protobuf/internal/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# source: tensorflow/core/framework/function.proto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (/home/aimlgpu2/.local/lib/python3.9/site-packages/google/protobuf/internal/__init__.py)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rApf00Woxyd6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#ignore warnings in the output\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1685335520264,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "wiDEob8fx1P8",
    "outputId": "81236c4c-7e82-4683-ed05-97534da8952d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4249137109399280529\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5736759296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17198852330855612843\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Check all available devices if GPU is available\n",
    "print(device_lib.list_local_devices())\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_UTTR458x2Rx"
   },
   "outputs": [],
   "source": [
    "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "# total number of our output classes: len(char_list)\n",
    "char_list = string.ascii_letters+string.digits\n",
    " \n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)\n",
    "        \n",
    "    return dig_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wpy6kolyx9MQ"
   },
   "outputs": [],
   "source": [
    "path = '/home/aimlgpu2/yash/mnt/ramdisk/max/90kDICT32px'\n",
    " \n",
    " \n",
    "# lists for training dataset\n",
    "training_img = []\n",
    "training_txt = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "orig_txt = []\n",
    " \n",
    "#lists for validation dataset\n",
    "valid_img = []\n",
    "valid_txt = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_orig_txt = []\n",
    " \n",
    "max_label_len = 0\n",
    " \n",
    "i =1 \n",
    "flag = 0\n",
    " \n",
    "for root, dirnames, filenames in os.walk(path):\n",
    " \n",
    "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
    "        # read input image and convert into gray scale image\n",
    "        img = cv2.cvtColor(cv2.imread(os.path.join(root, f_name)), cv2.COLOR_BGR2GRAY)   \n",
    " \n",
    "        # convert each image of shape (32, 128, 1)\n",
    "        w, h = img.shape\n",
    "        if h > 128 or w > 32:\n",
    "            continue\n",
    "        if w < 32:\n",
    "            add_zeros = np.ones((32-w, h))*255\n",
    "            img = np.concatenate((img, add_zeros))\n",
    " \n",
    "        if h < 128:\n",
    "            add_zeros = np.ones((32, 128-h))*255\n",
    "            img = np.concatenate((img, add_zeros), axis=1)\n",
    "        img = np.expand_dims(img , axis = 2)\n",
    "        \n",
    "        # Normalize each image\n",
    "        img = img/255.\n",
    "        \n",
    "        # get the text from the image\n",
    "        txt = f_name.split('_')[1]\n",
    "        \n",
    "        # compute maximum length of the text\n",
    "        if len(txt) > max_label_len:\n",
    "            max_label_len = len(txt)\n",
    "            \n",
    "           \n",
    "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
    "        if i%10 == 0:     \n",
    "            valid_orig_txt.append(txt)   \n",
    "            valid_label_length.append(len(txt))\n",
    "            valid_input_length.append(31)\n",
    "            valid_img.append(img)\n",
    "            valid_txt.append(encode_to_labels(txt))\n",
    "        else:\n",
    "            orig_txt.append(txt)   \n",
    "            train_label_length.append(len(txt))\n",
    "            train_input_length.append(31)\n",
    "            training_img.append(img)\n",
    "            training_txt.append(encode_to_labels(txt)) \n",
    "        \n",
    "        # break the loop if total data is 150000\n",
    "        if i == 160000:\n",
    "            flag = 1\n",
    "            break\n",
    "        i+=1\n",
    "    if flag == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jkGDRiH1yAIx"
   },
   "outputs": [],
   "source": [
    "# pad each output label to maximum text length\n",
    " \n",
    "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8gK-x3zkyCeU"
   },
   "outputs": [],
   "source": [
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    " \n",
    "# bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "act_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1685335563884,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "Pkvzkln4yF77",
    "outputId": "ada751ea-0673-4bbe-8f90-5fae0e2398a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 128, 1)]      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 128, 64)       640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 64, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 64, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 32, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 32, 256)        295168    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 32, 256)        590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 32, 256)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 32, 512)        1180160   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4, 32, 512)       2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 32, 512)        2359808   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 32, 512)       2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 32, 512)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 1, 31, 512)        1049088   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 31, 512)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 31, 256)          656384    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 31, 256)          394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 31, 63)            16191     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,619,711\n",
      "Trainable params: 6,617,663\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "act_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "executionInfo": {
     "elapsed": 1147,
     "status": "error",
     "timestamp": 1685335569795,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "g0udMQddyIka",
    "outputId": "a44280e0-9ec3-4ad7-9678-16c83448ec91"
   },
   "outputs": [],
   "source": [
    "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    " \n",
    " \n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    " \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    " \n",
    " \n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 1099,
     "status": "error",
     "timestamp": 1685335578809,
     "user": {
      "displayName": "yash chavan",
      "userId": "10144526596066718688"
     },
     "user_tz": -330
    },
    "id": "ALS5--RhyK2j",
    "outputId": "2c9c5c5a-e7f4-4a44-f372-8a34a6f25d7c"
   },
   "outputs": [],
   "source": [
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
    " \n",
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E3L_LvgsyNvK"
   },
   "outputs": [],
   "source": [
    "training_img = np.array(training_img)\n",
    "train_input_length = np.array(train_input_length)\n",
    "train_label_length = np.array(train_label_length)\n",
    "\n",
    "valid_img = np.array(valid_img)\n",
    "valid_input_length = np.array(valid_input_length)\n",
    "valid_label_length = np.array(valid_label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rL4GBNM8yRTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 26.6455\n",
      "Epoch 1: val_loss improved from inf to 24.57203, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 170s 272ms/step - loss: 26.6455 - val_loss: 24.5720\n",
      "Epoch 2/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 12.9827\n",
      "Epoch 2: val_loss improved from 24.57203 to 6.58563, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 12.9827 - val_loss: 6.5856\n",
      "Epoch 3/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 4.6370\n",
      "Epoch 3: val_loss improved from 6.58563 to 4.52827, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 153s 272ms/step - loss: 4.6370 - val_loss: 4.5283\n",
      "Epoch 4/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 3.4998\n",
      "Epoch 4: val_loss improved from 4.52827 to 3.61768, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 151s 268ms/step - loss: 3.4998 - val_loss: 3.6177\n",
      "Epoch 5/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 2.9731\n",
      "Epoch 5: val_loss improved from 3.61768 to 3.23450, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 2.9731 - val_loss: 3.2345\n",
      "Epoch 6/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 2.6287\n",
      "Epoch 6: val_loss improved from 3.23450 to 3.07831, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 158s 280ms/step - loss: 2.6287 - val_loss: 3.0783\n",
      "Epoch 7/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 2.3767\n",
      "Epoch 7: val_loss improved from 3.07831 to 2.97650, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 159s 282ms/step - loss: 2.3767 - val_loss: 2.9765\n",
      "Epoch 8/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 2.1708\n",
      "Epoch 8: val_loss improved from 2.97650 to 2.86014, saving model to best_model.hdf5\n",
      "563/563 [==============================] - 159s 283ms/step - loss: 2.1708 - val_loss: 2.8601\n",
      "Epoch 9/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 2.0435\n",
      "Epoch 9: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 159s 283ms/step - loss: 2.0435 - val_loss: 3.1987\n",
      "Epoch 10/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.9060\n",
      "Epoch 10: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 1.9060 - val_loss: 2.9259\n",
      "Epoch 11/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.7278\n",
      "Epoch 11: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 154s 274ms/step - loss: 1.7278 - val_loss: 2.8796\n",
      "Epoch 12/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.6042\n",
      "Epoch 12: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 1.6042 - val_loss: 2.9755\n",
      "Epoch 13/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.5382\n",
      "Epoch 13: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 1.5382 - val_loss: 2.8678\n",
      "Epoch 14/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.4607\n",
      "Epoch 14: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 1.4607 - val_loss: 2.9788\n",
      "Epoch 15/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.3413\n",
      "Epoch 15: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 159s 281ms/step - loss: 1.3413 - val_loss: 3.0350\n",
      "Epoch 16/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.2618\n",
      "Epoch 16: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 159s 282ms/step - loss: 1.2618 - val_loss: 2.9995\n",
      "Epoch 17/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.2116\n",
      "Epoch 17: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 160s 283ms/step - loss: 1.2116 - val_loss: 3.0483\n",
      "Epoch 18/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.1469\n",
      "Epoch 18: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 160s 285ms/step - loss: 1.1469 - val_loss: 3.0342\n",
      "Epoch 19/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.1004\n",
      "Epoch 19: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 285ms/step - loss: 1.1004 - val_loss: 3.1684\n",
      "Epoch 20/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 1.0235\n",
      "Epoch 20: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 1.0235 - val_loss: 3.2124\n",
      "Epoch 21/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.9585\n",
      "Epoch 21: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 0.9585 - val_loss: 3.1605\n",
      "Epoch 22/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.9314\n",
      "Epoch 22: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 0.9314 - val_loss: 3.4632\n",
      "Epoch 23/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.9397\n",
      "Epoch 23: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 0.9397 - val_loss: 3.2506\n",
      "Epoch 24/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.8637\n",
      "Epoch 24: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 160s 284ms/step - loss: 0.8637 - val_loss: 3.1228\n",
      "Epoch 25/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.8256\n",
      "Epoch 25: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 158s 281ms/step - loss: 0.8256 - val_loss: 3.2881\n",
      "Epoch 26/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7713\n",
      "Epoch 26: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 158s 281ms/step - loss: 0.7713 - val_loss: 3.2860\n",
      "Epoch 27/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7738\n",
      "Epoch 27: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 159s 282ms/step - loss: 0.7738 - val_loss: 3.3245\n",
      "Epoch 28/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.7611\n",
      "Epoch 28: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 158s 281ms/step - loss: 0.7611 - val_loss: 3.3994\n",
      "Epoch 29/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6931\n",
      "Epoch 29: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 158s 280ms/step - loss: 0.6931 - val_loss: 3.7127\n",
      "Epoch 30/30\n",
      "563/563 [==============================] - ETA: 0s - loss: 0.6780\n",
      "Epoch 30: val_loss did not improve from 2.86014\n",
      "563/563 [==============================] - 158s 280ms/step - loss: 0.6780 - val_loss: 3.4018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df0e954280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# gpu = gpus[0]\n",
    "\n",
    "# tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# from keras import backend as K\n",
    "# K.clear_session()\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hav4eMP0yUFl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "original_text =   Expend\n",
      "predicted text = Expend\n",
      "\n",
      "original_text =   RAKE\n",
      "predicted text = RAKE\n",
      "\n",
      "original_text =   IMAM\n",
      "predicted text = Imam\n",
      "\n",
      "original_text =   kraft\n",
      "predicted text = kralt\n",
      "\n",
      "original_text =   deceleration\n",
      "predicted text = deceleration\n",
      "\n",
      "original_text =   FOXHUNTING\n",
      "predicted text = FOXHUNTING\n",
      "\n",
      "original_text =   Renaud\n",
      "predicted text = Renaud\n",
      "\n",
      "original_text =   Trenchant\n",
      "predicted text = Trenchant\n",
      "\n",
      "original_text =   HOD\n",
      "predicted text = HOD\n",
      "\n",
      "original_text =   sculpt\n",
      "predicted text = sculpt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the saved best model weights\n",
    "act_model.load_weights('best_model.hdf5')\n",
    " \n",
    "# predict outputs on validation images\n",
    "prediction = act_model.predict(valid_img[:10])\n",
    " \n",
    "# use CTC decoder\n",
    "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                         greedy=True)[0][0])\n",
    " \n",
    "# see the results\n",
    "i = 0\n",
    "for x in out:\n",
    "    print(\"original_text =  \", valid_orig_txt[i])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    for p in x:  \n",
    "        if int(p) != -1:\n",
    "            print(char_list[int(p)], end = '')       \n",
    "    print('\\n')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNk1VzRERa94lb1G7+5LEaS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
